{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6qzT3P-HoPT"
   },
   "source": [
    "Copied from Neel's notebooks etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:06:51.977252Z",
     "start_time": "2023-05-31T11:06:51.891435Z"
    },
    "id": "2QRVBpLTHiqO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/kywm8cvn7hl8st6wskjzv8pr0000gn/T/ipykernel_77607/1856839453.py:22: DeprecationWarning:\n",
      "\n",
      "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "\n",
      "/var/folders/d_/kywm8cvn7hl8st6wskjzv8pr0000gn/T/ipykernel_77607/1856839453.py:23: DeprecationWarning:\n",
      "\n",
      "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Neel's Setup\n",
    "DEVELOPMENT_MODE = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    %pip install circuitsvis\n",
    "    %pip install git+https://github.com/neelnanda-io/neel-plotly\n",
    "    \n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:06:52.077360Z",
     "start_time": "2023-05-31T11:06:51.980695Z"
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K4amkcAIPHe",
    "outputId": "54c49fb7-24c2-492e-8c58-06c4f96f89b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'circuitsvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneel_plotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m line, scatter, imshow, histogram\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcircuitsvis\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcv\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'circuitsvis'"
     ]
    }
   ],
   "source": [
    "#@title Many useful imports\n",
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "# Import stuff\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import List, Union, Optional\n",
    "from jaxtyping import Float, Int\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from neel_plotly import line, scatter, imshow, histogram\n",
    "\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNH1UHjpIrUZ",
    "outputId": "5cff50e4-cdf5-45cc-b17d-d1da119df239"
   },
   "outputs": [],
   "source": [
    "#@title Load and run transformer lens model on device\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "gpt2_tokens = model.to_tokens(gpt2_text)\n",
    "print(gpt2_tokens.device)\n",
    "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfY9yZrnSIq0",
    "outputId": "dd796216-d7a1-4c4e-f020-246a896e11a3"
   },
   "outputs": [],
   "source": [
    "#@title Size-Constrained CircuitViz Attention Pattern\n",
    "# Plot doesn't display when initialized in a method ?\n",
    "layer = 2\n",
    "head = 4\n",
    "text = \"Access and plot the attention pattern of head L2H4 on the prompt\"\n",
    "cache = gpt2_cache\n",
    "\n",
    "class SizeLimitedObject:\n",
    "    def __init__(self, obj, max_width='500px', max_height='500px'):\n",
    "        self.obj = obj\n",
    "        self.max_width = max_width\n",
    "        self.max_height = max_height\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return f\"\"\"\n",
    "        <div style='max-width: {self.max_width}; max-height: {self.max_height}; padding: 20px;'>\n",
    "            {self.obj._repr_html_()}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "attention_pattern = cache[\"pattern\", layer, \"attn\"]\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "\n",
    "print(f\"Layer {layer} Head {head} Attention Pattern:\")\n",
    "head_attention = cv.attention.attention_pattern(tokens=str_tokens, attention=attention_pattern[head - 1])\n",
    "\n",
    "sized_viz = SizeLimitedObject(head_attention)\n",
    "# sized_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mrGBwha1lPhG"
   },
   "outputs": [],
   "source": [
    "#@title Ablation Hooks\n",
    "\n",
    "# Other inputs for utils.get_act_name can be found here:\n",
    "# https://github.com/luciaquirke/TransformerLens/blob/49edbec5424081182ef090265e2e6112153deffc/transformer_lens/utils.py\n",
    "\n",
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 8\n",
    "text = gpt2_text\n",
    "model = model\n",
    "\n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    # print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "def mlp_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "    value[:, :, :] = 0.\n",
    "    return value\n",
    "\n",
    "def get_mlp_mean_ablation_hook(batched_cache, act_name):\n",
    "    # mean over all batches\n",
    "    mean = batched_cache[act_name].mean(dim=0)\n",
    "    print(mean)\n",
    "\n",
    "    def mlp_mean_ablation_hook(\n",
    "        value: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "        hook: HookPoint\n",
    "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "        # print(f\"Shape of the value tensor: {value.shape}\")\n",
    "        value[:, :, :] = mean\n",
    "        return value\n",
    "\n",
    "    return mlp_mean_ablation_hook\n",
    "\n",
    "# original_loss = model(gpt2_tokens, return_type=\"loss\")\n",
    "# with model.hooks(fwd_hooks=[(utils.get_act_name(\"v\", layer_to_ablate), head_ablation_hook)]):\n",
    "#     ablated_head_loss = model(text, return_type=\"loss\")\n",
    "\n",
    "# with model.hooks(fwd_hooks=[(utils.get_act_name(\"pre\", layer_to_ablate), mlp_ablation_hook)]):\n",
    "#     ablated_mlp_loss = model(text, return_type=\"loss\")\n",
    "\n",
    "# print(f\"Original Loss: {original_loss.item():.3f}\")\n",
    "# print(f\"Ablated Head Loss: {ablated_head_loss.item():.3f}\")\n",
    "# print(f\"Ablated MLP Loss: {ablated_mlp_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "arxcAuoJp0kb"
   },
   "outputs": [],
   "source": [
    "#@title Print Tensor First Few Elements\n",
    "\n",
    "def head(tensor):\n",
    "    slices = [slice(0, 3) for _ in tensor.shape]\n",
    "    print(tensor[slices])\n",
    "\n",
    "def upper(tensor):\n",
    "    rows, cols = torch.tril_indices(tensor.size(-2), tensor.size(-1))\n",
    "    result = tensor[..., rows, cols]\n",
    "    print(result)\n",
    "\n",
    "# head(torch.rand(100, 100, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mA2ziaJt_rop"
   },
   "outputs": [],
   "source": [
    "#@title Line, Imshow/Heatmap, Scatter, Histogram\n",
    "\n",
    "def line(x, y, line_labels=None, xaxis=\"\", yaxis=\"\", title=\"\", **kwargs):\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    fig = px.line(df, x='x', y='y', title=title)\n",
    "    labels = {\"x\":xaxis, \"y\":yaxis}\n",
    "    if line_labels:\n",
    "        for c, label in enumerate(line_labels):\n",
    "            fig.data[c].name = label\n",
    "    fig.show()\n",
    "  \n",
    "def imshow(tensor, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    plot_kwargs = {\"color_continuous_scale\":\"RdBu\", \"color_continuous_midpoint\":0.0, \"labels\":{\"x\":xaxis, \"y\":yaxis}}\n",
    "    plot_kwargs.update(kwargs)\n",
    "    px.imshow(tensor, **plot_kwargs).show()\n",
    "\n",
    "def scatter(x, y, labels=None, xaxis=\"\", yaxis=\"\", title=\"\", **kwargs):\n",
    "    df = pd.DataFrame({xaxis: x, yaxis: y})\n",
    "    fig = px.scatter(df, x=xaxis, y=yaxis, title=title, **kwargs)\n",
    "    if labels:\n",
    "        for c, label in enumerate(labels):\n",
    "            fig.data[c].name = label\n",
    "    fig.show()\n",
    "\n",
    "def histogram(data):\n",
    "    df = pd.DataFrame({'Data': data})\n",
    "    fig = px.histogram(df, x='Data')\n",
    "    fig.show()\n",
    "\n",
    "# # histogram example\n",
    "# histogram(np.random.randn(1000))\n",
    "\n",
    "# # line examples\n",
    "# x = np.linspace(0, 50, 50, dtype=\"int\")\n",
    "# y = np.random.rand(50)\n",
    "# line(x, y, xaxis=\"x values\", yaxis=\"y values\", title=\"Gaussian Random Variables\")\n",
    "\n",
    "# x = np.linspace(0, 10, 100)\n",
    "# y = np.sin(x)\n",
    "# line(x, y, xaxis=\"x values\", yaxis=\"y values\", title=\"Sine Wave\")\n",
    "\n",
    "# # imshow example\n",
    "# tensor = np.random.rand(10, 10)\n",
    "# imshow(tensor, xaxis=\"X-axis\", yaxis=\"Y-axis\")\n",
    "\n",
    "# scatter example\n",
    "# x = np.linspace(0, 50, 50, dtype=\"int\")\n",
    "# y = np.random.rand(50)\n",
    "# scatter(x, y, xaxis=\"x values\", yaxis=\"y values\", title=\"Scatter Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "referenced_widgets": [
      "77645a786fb848daa52292123f400757",
      "d404c75521da43e897db824ce2ee2e8c",
      "c3242a3bd78d42c2bb23a0a42ff6b27b",
      "e11146730ad147aeb3d4b72236043d77",
      "22cf64c1d5374d3f9ce6a04630e08741",
      "aa6a7e83eabe4eae8e145ebbd8ddd7da",
      "6fb7786473ce413e95e6508a724a305c",
      "248dcada484e44b1ba8565ceaa3be716",
      "3eac9743e5164c9bb4d434a989da3e3d",
      "922ccf8ebe0144beb5ed5b66e9d822c5",
      "2404b2dceca84d68b8cd04883db5ce3b"
     ]
    },
    "id": "__AY5Hv0gMXY",
    "outputId": "7f3ac510-3457-4fb2-f4c6-5ea277d67ffe"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = load_dataset('ag_news', split='test')\n",
    "\n",
    "# # find maximum number of tokens\n",
    "# def find_length(example):\n",
    "#     tokens = model.to_tokens(example['text'])\n",
    "#     return {\"length\": tokens.shape[1]}\n",
    "# dataset = dataset.map(find_length)\n",
    "# max_length = max(dataset['length']) # 260\n",
    "max_length = 260\n",
    "\n",
    "# Find number of entries at each pos\n",
    "pos_counts = torch.zeros(max_length)\n",
    "def add_entries(example):\n",
    "    tokens = model.to_tokens(example['text'])\n",
    "    pos_counts[:len(tokens[0])] += 1\n",
    "dataset.map(add_entries)\n",
    "print(pos_counts.shape)\n",
    "\n",
    "act_name = utils.get_act_name(\"pre\", 0)\n",
    "\n",
    "test_text = dataset[0].get('text')\n",
    "tokens = model.to_tokens(test_text)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# get pos counts over all batches, then use to get average activation at each position and dimension. \n",
    "# Or histogram of token lists and filter / truncate to a reasonable length\n",
    "\n",
    "batch_size = 10\n",
    "sum_acts = torch.zeros(1, max_length, model.cfg.d_mlp) # batch, pos, d_mlp\n",
    "\n",
    "for i in range(0, 7590, batch_size):\n",
    "    test_text = dataset[i, i + batch_size].get('text')[0]\n",
    "    tokens = model.to_tokens(test_text)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "\n",
    "    # Keep track of how many pos aren't 0 for averaging\n",
    "\n",
    "    # add zeros for the unused tensors.\n",
    "    acts = cache[act_name]\n",
    "    padded_acts = torch.nn.functional.pad(acts, (0, 0, 0, sum_acts.shape[1]-acts.shape[1]))\n",
    "\n",
    "    sum_acts += padded_acts[0]\n",
    "\n",
    "    # another strategy: divide each act by the number of tokens at that position in the dataset, then add to average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XymXKOoLS2sk",
    "outputId": "1289f193-67b8-42c9-c9b4-9abc2bce792e"
   },
   "outputs": [],
   "source": [
    "pos_counts_expanded = pos_counts.unsqueeze(0).unsqueeze(-1)  # [1, pos, 1]\n",
    "print(pos_counts[])\n",
    "print(pos_counts_expanded.shape)\n",
    "# print(pos_counts_expanded)\n",
    "\n",
    "print(sum_acts.shape) # [batch, pos, d_mlp]\n",
    "print(sum_acts.mean(0))\n",
    "print(sum_acts[0][0][:5])\n",
    "print(sum_acts / pos_counts_expanded)\n",
    "\n",
    "\n",
    "# # gpt-small\n",
    "# gpt2_tokens = gpt2_model.to_tokens(test_text)\n",
    "# gpt_loss, gpt_cache = gpt2_model.run_with_cache(gpt2_tokens, return_type=\"loss\")\n",
    "# mean_ablated_gpt_loss = gpt2_model.run_with_hooks(\n",
    "#     gpt2_tokens,\n",
    "#     return_type=\"loss\",\n",
    "#     fwd_hooks=[(utils.get_act_name(\"pre\", 0), get_mlp_mean_ablation_hook(gpt_cache, utils.get_act_name(\"pre\", 0)))])\n",
    "\n",
    "# print(mean_ablated_gpt_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "22cf64c1d5374d3f9ce6a04630e08741": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "2404b2dceca84d68b8cd04883db5ce3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "248dcada484e44b1ba8565ceaa3be716": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3eac9743e5164c9bb4d434a989da3e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fb7786473ce413e95e6508a724a305c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77645a786fb848daa52292123f400757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d404c75521da43e897db824ce2ee2e8c",
       "IPY_MODEL_c3242a3bd78d42c2bb23a0a42ff6b27b",
       "IPY_MODEL_e11146730ad147aeb3d4b72236043d77"
      ],
      "layout": "IPY_MODEL_22cf64c1d5374d3f9ce6a04630e08741"
     }
    },
    "922ccf8ebe0144beb5ed5b66e9d822c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa6a7e83eabe4eae8e145ebbd8ddd7da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3242a3bd78d42c2bb23a0a42ff6b27b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_248dcada484e44b1ba8565ceaa3be716",
      "max": 7600,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3eac9743e5164c9bb4d434a989da3e3d",
      "value": 7600
     }
    },
    "d404c75521da43e897db824ce2ee2e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa6a7e83eabe4eae8e145ebbd8ddd7da",
      "placeholder": "​",
      "style": "IPY_MODEL_6fb7786473ce413e95e6508a724a305c",
      "value": "Map:  99%"
     }
    },
    "e11146730ad147aeb3d4b72236043d77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_922ccf8ebe0144beb5ed5b66e9d822c5",
      "placeholder": "​",
      "style": "IPY_MODEL_2404b2dceca84d68b8cd04883db5ce3b",
      "value": " 7525/7600 [00:06&lt;00:00, 1637.81 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
